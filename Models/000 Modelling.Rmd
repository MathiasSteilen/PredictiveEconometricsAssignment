---
title: "Predictive Econometrics: Individual Task - Modelling Parent File"
output: 
  html_document:
    theme: simplex
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
    code_folding: show
editor_options: 
  chunk_output_type: console
---

### Admin

```{r setup, include=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()[[2]]))

library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(doParallel)
library(vip)
library(broom)
library(GGally)
library(car)
library(stacks)

# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(where(is.character), as.factor))

rm(obesity)

glimpse(data)
```

***
### Stratified Resampling vs. Non-Stratified Resampling
***

Strata: BMI

```{r class.source = 'fold-show'}
set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)

eval_metrics <- metric_set(rsq, rmse, mape, mae)
```

```{r LIN1}
source("LIN1.R")
```

```{r class.source = 'fold-show'}
set.seed(666)
dt_split <- data %>% 
  initial_split()

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5)

eval_metrics <- metric_set(rsq, rmse, mape, mae)
```

```{r LIN1}
source("LIN1.R")
```

Evaluating performance of the two:

```{r}
read_rds("LIN1_fit_strata.rds") %>% 
  collect_predictions() %>% 
  rsq(truth = BMI, estimate = .pred)

read_rds("LIN1_fit_nostrata.rds") %>% 
  collect_predictions() %>% 
  rsq(truth = BMI, estimate = .pred)
```

Conclusion: Stratified Resampling works slightly better, with $R^2 = 0.517$, which is bigger than the one without stratified resampling, which has $R^2 = 0.497$ (- 2 ppts)

***
### Encoding ordinal columns to factors vs. not doing that
***

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  # mutate(across(c(FCVC, NCP, CH2O, FAF, TUE), as.factor))
  mutate(across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)
```

```{r LIN1}
source("LIN1.R")
source("GB1.R")
source("SVM1.R")
```

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(FCVC, NCP, CH2O, FAF, TUE), as.factor),
         across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)
```

```{r LIN1}
source("LIN1.R")
source("GB1.R")
source("SVM1.R")
```

Encoding as factors makes:
- EN slightly better; 0.491 -> 0.517
- GB slightly worse; 0.858 -> 0.852
- SVM slightly better; 0.822 -> 0.840

Conclusion: Change to factors for SVM and EN

***
### Encoding ordinal factors to numeric vs. not doing that
***

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(where(is.character), as.factor))
  # mutate(across(c(CAEC, CALC), 
  #               ~ case_when(
  #                 .x == "no" ~ 0,
  #                 .x == "Sometimes" ~ 1,
  #                 .x == "Frequently" ~ 2,
  #                 .x == "Always" ~ 3
  #               )),
  #        across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)
```

```{r LIN1}
source("LIN1.R")
source("GB1.R")
source("SVM1.R")
```

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(CAEC, CALC), 
                ~ case_when(
                  .x == "no" ~ 0,
                  .x == "Sometimes" ~ 1,
                  .x == "Frequently" ~ 2,
                  .x == "Always" ~ 3
                )),
         across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)
```

```{r LIN1}
source("LIN1.R")
source("GB1.R")
source("SVM1.R")
```

Encoding as ordinal makes:
- OLS slightly worse; 0.491 -> 0.441
- GB better; 0.858 -> 0.861
- SVM slightly worse; 0.822 -> 0.813

Conclusion: Only for GB

***
### Running the Tuning and Fitting Process
***

First configuration (preprocessing for SVM and EN):

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(FCVC, NCP, CH2O, FAF, TUE), as.factor),
         across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)

eval_metrics <- metric_set(rsq, rmse, mape, mae)
```

```{r}
source("LIN1.R")
source("LIN2.R")
source("LIN3.R")
source("LIN4.R")
source("LIN5.R")
source("LIN6.R")
source("LIN7.R")
```

```{r}
source("SVM1.R")
source("SVM2.R")
source("SVM3.R")
source("SVM4.R")
source("SVM5.R")
source("SVM6.R")
```

Second: Preprocessing for GB

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(CAEC, CALC), 
                ~ case_when(
                  .x == "no" ~ 0,
                  .x == "Sometimes" ~ 1,
                  .x == "Frequently" ~ 2,
                  .x == "Always" ~ 3
                )),
         across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)

eval_metrics <- metric_set(rsq, rmse, mape, mae)
```

```{r}
source("GB1.R")
source("GB2.R")
source("GB3.R")
source("GB4.R")
source("GB5.R")
```

```{r}
gb1_fit <- readRDS("GB1_fit.rds")
gb2_fit <- readRDS("GB2_fit.rds")
gb3_fit <- readRDS("GB3_fit.rds")
gb4_fit <- readRDS("GB4_fit.rds")
gb5_fit <- readRDS("GB5_fit.rds")
```

```{r}
bind_rows(
  gb1_fit %>% 
    collect_predictions() %>% 
    mutate(model = "gb1"),
  gb2_fit %>% 
    collect_predictions() %>% 
    mutate(model = "gb2"),
  gb3_fit %>% 
    collect_predictions() %>% 
    # Convert back from log transform
    mutate(.pred = exp(.pred)) %>% 
    mutate(model = "gb3"),
  gb4_fit %>% 
    collect_predictions() %>% 
    mutate(model = "gb4"),
  gb5_fit %>% 
    collect_predictions() %>% 
    mutate(model = "gb5")
) %>% 
  ggplot(aes(BMI, .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline() +
  facet_wrap(~ model, scales = "free", ncol = 6)
```

***
### Building A Stacked Model
***

```{r}
# Reading in the data
# Loading the data
load("../Data/obesity.Rdata")

# Manipulations upon import
data <- obesity %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(CAEC, CALC), 
                ~ case_when(
                  .x == "no" ~ 0,
                  .x == "Sometimes" ~ 1,
                  .x == "Frequently" ~ 2,
                  .x == "Always" ~ 3
                )),
         across(where(is.character), as.factor))

rm(obesity)

glimpse(data)

set.seed(666)
dt_split <- data %>% 
  initial_split(strata = BMI)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(666)
folds <- vfold_cv(dt_train, v = 5, strata = BMI)

eval_metrics <- metric_set(rsq, rmse, mape, mae)
```

```{r}
source("GB1.R")
```

```{r, eval=FALSE}
blended_gb <- stacks() %>% 
  add_candidates(readRDS("GB1_tune.rds")) %>% 
  blend_predictions() %>% 
  fit_members()

saveRDS(blended_gb, "blended_gb.rds")
```

```{r}
blended_gb <- readRDS("blended_gb.rds")
```

Compare blended model oos metrics to models before

```{r}
blended_gb %>% 
  predict(dt_test) %>% 
  bind_cols(dt_test %>% select(BMI)) %>% 
  eval_metrics(estimate = .pred, truth = BMI) %>% 
  write_csv("blended_gb_metrics.csv")
```

***
### Make Predictions for Submission
***

Select the best hyperparameters from tuning, then fit that GB model on the entire available data instead of only data train and then make predictions on the preprocessed holdout without the target variable and store as csv with one column.

```{r}
gb1_tune <- readRDS("GB1_tune.rds")
gb1_tune %>% show_best(metric = "rsq") %>% head(1) %>% glimpse()

load("../Data/obesity_predict.Rdata")
obesity_predict_processed <- obesity_predict %>% 
  as_tibble() %>% 
  distinct() %>% 
  mutate(across(c(CAEC, CALC), 
                ~ case_when(
                  .x == "no" ~ 0,
                  .x == "Sometimes" ~ 1,
                  .x == "Frequently" ~ 2,
                  .x == "Always" ~ 3
                )),
         across(where(is.character), as.factor))

# Recipe: Data Preprocessing
gb_rec <- recipe(BMI ~ .,
                 data = dt_train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors())

# Model Specification
gb_spec <- boost_tree(trees = tune(),
                      tree_depth = tune(),
                      min_n = tune(),
                      loss_reduction = tune(),
                      sample_size = tune(),
                      mtry = tune(),
                      learn_rate = tune()) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("regression")

# Workflow
gb_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(gb_spec)

# Fit on the entire dataset
gb_final_fit <- gb_wflow %>% 
  finalize_workflow(select_best(gb1_tune, metric = "rsq")) %>% 
  fit(data)

gb_final_fit %>% 
  augment(obesity_predict_processed) %>% 
  transmute(BMI = .pred) %>% 
  write_csv("../predictions_submission.csv")
```